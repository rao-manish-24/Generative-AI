{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4da4a8",
   "metadata": {},
   "source": [
    "# Lecture 5 Transformer - Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7cb38",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a9b83",
   "metadata": {},
   "source": [
    "Positional encoding is a technique used to add positional information to input embeddings in Transformer models, allowing the model to understand the order of tokens in a sequence (since Transformers are order-agnostic by default). Below are three Python coding examples that progressively illustrate how positional encoding works:\n",
    "- Basic Positional Encoding (Sine & Cosine) from Scratch\n",
    "- Positional Encoding in a Simple Transformer Model\n",
    "- Learnable Positional Encoding Using PyTorch (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe9c4f",
   "metadata": {},
   "source": [
    "#### 2 Positional Encoding in a Simple Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae2150",
   "metadata": {},
   "source": [
    "This example demonstrates how positional encoding can be integrated into a simplified Transformer model. We'll use TensorFlow for this example, where positional encoding is added to token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "480f2ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding Added to Embeddings:\n",
      " tf.Tensor(\n",
      "[[[ 2.59246118e-02  9.60563362e-01  2.55688466e-02  9.98157859e-01\n",
      "    4.14471701e-03  9.88600671e-01  1.85531639e-02  1.04581964e+00\n",
      "   -4.60795760e-02  1.02870810e+00 -3.48631851e-02  1.02515185e+00\n",
      "   -2.47522127e-02  9.95232642e-01  5.76246530e-04  9.75355029e-01]\n",
      "  [ 8.48161519e-01  5.05879521e-01  2.99847782e-01  9.35095847e-01\n",
      "    7.62018412e-02  9.45547402e-01  6.55159950e-02  9.99528527e-01\n",
      "   -2.64675505e-02  9.73422885e-01  3.51169705e-03  1.01324773e+00\n",
      "    7.86558352e-03  1.01808608e+00  8.14105384e-04  9.69907045e-01]\n",
      "  [ 9.09123003e-01 -4.21145171e-01  5.96670270e-01  8.48561585e-01\n",
      "    1.90529138e-01  9.95508492e-01  3.66190299e-02  1.00501430e+00\n",
      "    2.57118307e-02  9.63452518e-01 -1.51995234e-02  9.69877839e-01\n",
      "    1.56975002e-03  1.04330480e+00 -3.15220356e-02  9.93340850e-01]\n",
      "  [ 1.22421324e-01 -1.01490378e+00  8.16063583e-01  6.04021251e-01\n",
      "    3.17716688e-01  9.65690732e-01  6.61123842e-02  1.00374174e+00\n",
      "    4.06109318e-02  1.02941680e+00  3.19658108e-02  1.01499939e+00\n",
      "   -5.88131230e-03  1.04671919e+00 -4.55705747e-02  1.01914811e+00]\n",
      "  [-7.64846563e-01 -6.79876387e-01  9.52635884e-01  2.55114019e-01\n",
      "    3.44932407e-01  8.93250287e-01  1.02002785e-01  1.01136804e+00\n",
      "    2.55754516e-02  9.61938202e-01  1.93338562e-03  1.01681173e+00\n",
      "    4.76775840e-02  9.78159606e-01 -9.04662162e-03  9.84930158e-01]]\n",
      "\n",
      " [[-4.00481448e-02  1.01698732e+00  4.83838059e-02  1.04679990e+00\n",
      "   -2.83998139e-02  9.57045138e-01 -8.90707970e-03  1.02067244e+00\n",
      "   -2.65717395e-02  1.01799214e+00  4.12106253e-02  1.00805056e+00\n",
      "   -9.59746912e-03  9.99452353e-01  2.34151967e-02  1.01464748e+00]\n",
      "  [ 8.36444676e-01  5.20625234e-01  3.09734225e-01  9.96370256e-01\n",
      "    1.23865381e-01  1.02096224e+00  8.62034224e-03  1.02044189e+00\n",
      "   -2.91062966e-02  1.03908396e+00 -3.12531367e-03  9.56004143e-01\n",
      "   -4.62929383e-02  9.55355525e-01  4.23969179e-02  1.00407267e+00]\n",
      "  [ 9.30172443e-01 -4.07199442e-01  5.98622084e-01  8.50627005e-01\n",
      "    2.46989802e-01  1.01815736e+00  1.08967453e-01  1.02938128e+00\n",
      "    2.24521235e-02  1.01673639e+00 -3.99872428e-03  9.91304040e-01\n",
      "    3.99058014e-02  1.00316334e+00 -4.60665934e-02  9.94888246e-01]\n",
      "  [ 1.04457974e-01 -9.43035245e-01  8.10337245e-01  5.93056977e-01\n",
      "    2.51170576e-01  9.63127851e-01  6.88899085e-02  1.04069841e+00\n",
      "    2.22284719e-03  1.03401077e+00  5.33556528e-02  1.02837968e+00\n",
      "    3.91908363e-02  9.61005926e-01  4.98782583e-02  1.01740575e+00]\n",
      "  [-7.86981285e-01 -6.28463507e-01  9.41001892e-01  3.38431835e-01\n",
      "    3.61899287e-01  9.25676286e-01  1.01585127e-01  9.92942512e-01\n",
      "    7.63258412e-02  9.95642364e-01  2.03162916e-02  1.00094926e+00\n",
      "   -3.97885516e-02  9.71550047e-01 -2.77760345e-02  9.56420600e-01]]], shape=(2, 5, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class TransformerWithPositionalEncoding(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, seq_length, d_model):\n",
    "        super(TransformerWithPositionalEncoding, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = self.compute_positional_encoding(seq_length, d_model)\n",
    "\n",
    "    def compute_positional_encoding(self, seq_length, d_model):\n",
    "        \"\"\"\n",
    "        Computes positional encoding using sine and cosine functions.\n",
    "        \"\"\"\n",
    "        angle_rads = np.arange(seq_length)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / d_model)\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Apply sin to even indices in the array\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Apply cos to odd indices in the array\n",
    "        \n",
    "        return tf.cast(angle_rads, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Adds positional encoding to the embedded input sequence.\n",
    "        \"\"\"\n",
    "        # Add positional encoding to the input embeddings\n",
    "        seq_length = tf.shape(x)[1]\n",
    "        x_embed = self.embedding(x)\n",
    "        x_position = self.positional_encoding[:seq_length, :]\n",
    "        x_embed += x_position\n",
    "        return x_embed\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 50 # max 50 words\n",
    "seq_length = 10 # max 10 words in a sentence\n",
    "d_model = 16 # Each word is vectorized into 1 x 16. \n",
    "\n",
    "# Sample input: batch of sequences with token indices\n",
    "# Each number can be regarded as a word and vectorized into a 1 x 16 array\n",
    "input_sequence = tf.constant([[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]])\n",
    "\n",
    "# Create the Transformer model with positional encoding\n",
    "transformer_model = TransformerWithPositionalEncoding(vocab_size, seq_length, d_model)\n",
    "\n",
    "# Forward pass\n",
    "output = transformer_model(input_sequence)\n",
    "print(\"Positional Encoding Added to Embeddings:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e9c39",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Embedding Layer: The embedding layer converts the input token indices into dense vectors (embeddings).\n",
    "- Sine & Cosine Positional Encoding: The compute_positional_encoding method uses sine and cosine functions to compute the positional encodings.\n",
    "- Forward Pass (call method): The positional encodings are added to the token embeddings, injecting positional information into the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecbd03c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
