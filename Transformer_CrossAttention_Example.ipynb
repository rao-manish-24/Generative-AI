{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce210ca",
   "metadata": {},
   "source": [
    "# Lecture 5 Transformer - Cross Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb6671",
   "metadata": {},
   "source": [
    "To calculate cross-attention between the decoder and the encoder in a transformer model from scratch, we'll need to implement key steps of the attention mechanism, particularly focusing on how attention weights are computed between a query (from the decoder) and key-value pairs (from the encoder). Here's an example of how this can be done using Python and NumPy:\n",
    "\n",
    "**Overview of Cross-Attention**\n",
    "- Query (Q) comes from the decoder.\n",
    "- Key (K) and Value (V) come from the encoder.\n",
    "- Compute the attention scores as the dot product between Q and K.\n",
    "- Scale the attention scores by dividing by the square root of the dimensionality of the keys.\n",
    "- Apply the softmax function to obtain the attention weights.\n",
    "- Use these attention weights to get a weighted sum of the V (values) to produce the final context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper function for softmax\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # subtract max for numerical stability\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Function to compute cross-attention\n",
    "def cross_attention(Q, K, V, d_k):\n",
    "    # Step 1: Compute raw attention scores (Q * K^T)\n",
    "    attention_scores = Q @ K.T  \n",
    "    #(seq_len_dec x d_model)(d_model x seq_len_enc)\n",
    "    \n",
    "    # Step 2: Scale the attention scores by sqrt(d_k)\n",
    "    attention_scores /= np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply softmax to get attention weights\n",
    "    attention_weights = softmax(attention_scores)\n",
    "     #(seq_len_dec x seq_len_enc)\n",
    "    \n",
    "    # Step 4: Compute the final output (attention_weights * V)\n",
    "    attention_output = attention_weights @ V\n",
    "    # (seq_len_dec x seq_len_enc)(seq_len_enc x d_model)\n",
    "    # seq_len_dec x d_model\n",
    "    \n",
    "    return attention_output, attention_weights\n",
    "\n",
    "# Example dimensions\n",
    "batch_size = 1    # Single example\n",
    "seq_len_enc = 5   # Encoder sequence length\n",
    "seq_len_dec = 3   # Decoder sequence length\n",
    "d_model = 4       # Model dimensionality\n",
    "d_k = d_model     # Key dimension (same as model dimensionality here)\n",
    "\n",
    "# Example encoder output (K, V) and decoder input (Q)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Encoder output (K, V)\n",
    "K = np.random.rand(seq_len_enc, d_model)  # Key matrix from encoder\n",
    "V = np.random.rand(seq_len_enc, d_model)  # Value matrix from encoder\n",
    "\n",
    "# Decoder input (Q)\n",
    "Q = np.random.rand(seq_len_dec, d_model)  # Query matrix from decoder\n",
    "\n",
    "# Compute cross-attention\n",
    "attention_output, attention_weights = cross_attention(Q, K, V, d_k)\n",
    "\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"\\nAttention Output (context vectors):\\n\", attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b23fa5",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Q (Query) represents the input from the decoder, which attends to the encoder's output.\n",
    "- K (Key) and V (Value) are derived from the encoder's output.\n",
    "- The dot product of Q and K gives us the attention scores.\n",
    "- These scores are scaled by the square root of the key's dimension (d_k) to stabilize gradients.\n",
    "- Softmax is applied to convert scores into probabilities (attention weights).\n",
    "Finally, the attention weights are used to compute a weighted sum of the value vectors, resulting in the context vector, which will be passed back to the decoder.<br>\n",
    "\n",
    "**Output:**\n",
    "- Attention Weights: Probability distribution over the encoder's sequence, representing how much focus the decoder puts on each encoder token.\n",
    "- Attention Output: Contextualized vectors for each decoder token based on the encoder's output.\n",
    "\n",
    "This is a simplified version without multi-head attention and other optimizations. The core idea of cross-attention is to align the decoder's queries with the encoder's keys to extract relevant information (values) from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455fe69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
