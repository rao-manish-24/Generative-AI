{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f521a7e",
   "metadata": {},
   "source": [
    "# Lecture 4 Self Attention - Masked Attention\n",
    "## Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76b74e",
   "metadata": {},
   "source": [
    "#### Masked Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bce31",
   "metadata": {},
   "source": [
    "Masked attention is a crucial mechanism in transformer architectures, particularly in tasks like language modeling, where it's essential to prevent the model from accessing future tokens during training. This ensures that the predictions for a particular position depend only on the known outputs at positions before it.\n",
    "\n",
    "Below are two Python examples demonstrating masked attention:\n",
    "- From Scratch Implementation Using NumPy\n",
    "- Using PyTorch's Built-in Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d62b20",
   "metadata": {},
   "source": [
    "#### 2. Masked Attention Using PyTorch\n",
    "Leveraging PyTorch's built-in nn.MultiheadAttention module simplifies the implementation of masked attention. PyTorch allows the creation of a causal mask to ensure that each position can only attend to previous positions and itself.\n",
    "\n",
    "Step-by-Step Explanation\n",
    "- Import Libraries and Set Parameters:\n",
    "    - Define dimensions, number of heads, and other necessary parameters.\n",
    "- Initialize the MultiheadAttention Module:\n",
    "    - Create an instance of nn.MultiheadAttention with the specified embedding dimension and number of heads.\n",
    "-Prepare Input Data:\n",
    "    - PyTorch expects input shapes as (seq_length, batch_size, embedding_dim) unless batch_first=True is set.\n",
    "    - Initialize random input tensors for queries, keys, and values.\n",
    "- Create Causal Mask:\n",
    "    - Generate a mask that prevents each position from attending to future positions.\n",
    "- Perform Multi-Head Attention:\n",
    "    - Pass the queries, keys, and values to the forward method of the MultiheadAttention module along with the mask.\n",
    "    - Obtain the output and attention weights.\n",
    "- Output the Results:\n",
    "    - Display the attention output and the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359fe05c",
   "metadata": {},
   "source": [
    "**Explanation of Output**\n",
    "- Input X: A randomly generated input tensor with shape (batch_size, seq_length, embedding_dim).\n",
    "- Attention Weights: After applying the mask, each position can only attend to itself and previous positions. For example, the first position attends only to itself, the second attends to the first and second, and so on.\n",
    "- Output: The result of the masked attention mechanism, maintaining the same shape as the input.\n",
    "\n",
    "Note: The actual numerical values may vary slightly due to random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b547af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X:\n",
      " tensor([[[0.6855, 0.9696, 0.4295, 0.4961, 0.3849, 0.0825, 0.7400, 0.0036],\n",
      "         [0.8104, 0.8741, 0.9729, 0.3821, 0.0892, 0.6124, 0.7762, 0.0023],\n",
      "         [0.3865, 0.2003, 0.4563, 0.2539, 0.2956, 0.3413, 0.0248, 0.9103],\n",
      "         [0.9192, 0.4216, 0.4431, 0.2959, 0.0485, 0.0134, 0.6858, 0.2255]]])\n",
      "\n",
      "Attention Weights:\n",
      " tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5110, 0.4890, 0.0000, 0.0000],\n",
      "         [0.3340, 0.3238, 0.3422, 0.0000],\n",
      "         [0.2523, 0.2368, 0.2546, 0.2563]]], grad_fn=<MeanBackward1>)\n",
      "\n",
      "Attention Output:\n",
      " tensor([[[    -0.0897,     -0.2322,      0.0044,      0.1931,     -0.2976,\n",
      "              -0.0187,     -0.1550,     -0.0145],\n",
      "         [    -0.1037,     -0.2087,     -0.0187,      0.1829,     -0.3419,\n",
      "              -0.0234,     -0.2459,      0.0579],\n",
      "         [    -0.0225,     -0.2682,      0.0002,      0.0759,     -0.2667,\n",
      "              -0.0839,     -0.2246,     -0.0096],\n",
      "         [    -0.0267,     -0.2767,      0.0029,      0.0618,     -0.2711,\n",
      "              -0.0982,     -0.1954,     -0.0231]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "batch_size = 1\n",
    "seq_length = 4\n",
    "embedding_dim = 8\n",
    "num_heads = 2\n",
    "\n",
    "# Initialize MultiheadAttention\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim = embedding_dim, num_heads = num_heads, batch_first=True)\n",
    "\n",
    "# Random input\n",
    "X = torch.rand(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "# In PyTorch's MultiheadAttention, when batch_first=True, input shape is (batch_size, seq_length, embedding_dim)\n",
    "# Otherwise, it should be (seq_length, batch_size, embedding_dim)\n",
    "\n",
    "# Create a causal mask to prevent attention to future tokens\n",
    "# The mask shape should be (seq_length, seq_length)\n",
    "mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()  # Upper triangular matrix\n",
    "\n",
    "# Perform Multi-Head Attention\n",
    "# Using the same tensor for queries, keys, and values (self-attention)\n",
    "attn_output, attn_weights = multihead_attn(X, X, X, attn_mask = mask)\n",
    "\n",
    "# Display Results\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "print(\"Input X:\\n\", X)\n",
    "print(\"\\nAttention Weights:\\n\", attn_weights)\n",
    "print(\"\\nAttention Output:\\n\", attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d521445",
   "metadata": {},
   "source": [
    "**Explanation of Output**\n",
    "- Input X: A randomly generated input tensor with shape (batch_size, seq_length, embedding_dim).\n",
    "- Attention Output: Each position in the sequence has been transformed based on the masked attention mechanism. The output maintains the same shape as the input.\n",
    "- Attention Weights: The attention scores for each head. Due to the mask, positions can only attend to themselves and previous positions. For instance, the first token only attends to itself, the second token attends to the first and second, and so on. In this random example, the attention weights are uniformly distributed among the allowed positions.\n",
    "\n",
    "Note: The actual numerical values may vary slightly due to random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7571b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
