{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135ab6b1",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f100cb2e",
   "metadata": {},
   "source": [
    "Let’s walk through a simplified example of a Generative Adversarial Network (GAN), which consists of two neural networks: a Generator and a Discriminator. The generator tries to create fake data that looks real, while the discriminator tries to distinguish between real and fake data. Over time, the generator gets better at creating realistic data, and the discriminator improves at detecting it.\n",
    "\n",
    "Here's an overview of the steps we’ll take:\n",
    "- Define the Generator and Discriminator Networks.\n",
    "- Set Up the Training Loop where both networks train in tandem.\n",
    "- Use Random Noise to Generate Fake Data through the generator.\n",
    "- Train the Discriminator to classify real vs. fake data.\n",
    "- Update the Generator to produce more convincing fake data based on the discriminator's feedback.\n",
    "\n",
    "Below is a simple GAN example using PyTorch. This example will train a GAN on random 1-dimensional data, which can be extended to images or other types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b228e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss D: 0.7153, Loss G: 0.5921\n",
      "Epoch 500, Loss D: 0.6808, Loss G: 0.6407\n",
      "Epoch 1000, Loss D: 0.6957, Loss G: 0.6884\n",
      "Epoch 1500, Loss D: 0.6938, Loss G: 0.6927\n",
      "Epoch 2000, Loss D: 0.6883, Loss G: 0.7358\n",
      "Epoch 2500, Loss D: 0.6970, Loss G: 0.6516\n",
      "Epoch 3000, Loss D: 0.6953, Loss G: 0.7007\n",
      "Epoch 3500, Loss D: 0.7063, Loss G: 0.6558\n",
      "Epoch 4000, Loss D: 0.6985, Loss G: 0.7090\n",
      "Epoch 4500, Loss D: 0.6924, Loss G: 0.7103\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Step 1: Define Generator and Discriminator Networks\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Tanh()  # Outputs values in the range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()  # Outputs probability [0, 1], real (1) or fake (0).\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Step 2: Initialize Networks, Optimizers, and Loss Function\n",
    "latent_dim = 10  # Size of the noise vector input to the Generator\n",
    "data_dim = 1     # Output dimension of data\n",
    "generator = Generator(latent_dim, data_dim)\n",
    "discriminator = Discriminator(data_dim)\n",
    "\n",
    "# Use Binary Cross-Entropy loss and separate optimizers for each network\n",
    "# Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, maximize=False)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, maximize=False)\n",
    "\n",
    "# Step 3: Training Loop\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    # Train Discriminator\n",
    "    # -------------------\n",
    "\n",
    "    # Generate real data (random numbers centered around 0.5, ranging from 0.25 to 0.75)\n",
    "    real_data = torch.rand(64, data_dim) * 0.5 + 0.25\n",
    "    real_labels = torch.ones(64, 1)\n",
    "\n",
    "    # Generate fake data using the generator\n",
    "    noise = torch.randn(64, latent_dim) # 10 series of noise\n",
    "    fake_data = generator(noise)\n",
    "    fake_labels = torch.zeros(64, 1)\n",
    "\n",
    "    # Calculate discriminator loss on real and fake data\n",
    "    real_output = discriminator(real_data)\n",
    "    fake_output = discriminator(fake_data.detach())  # detach to avoid training the generator here\n",
    "    loss_real = criterion(real_output, real_labels)\n",
    "    loss_fake = criterion(fake_output, fake_labels)\n",
    "    loss_d = (loss_real + loss_fake)/2\n",
    "\n",
    "    # Backpropagate and optimize discriminator\n",
    "    optimizer_d.zero_grad()\n",
    "    loss_d.backward()\n",
    "    optimizer_d.step()\n",
    "\n",
    "    # Train Generator\n",
    "    # ----------------\n",
    "\n",
    "    # Generate fake data again for updating the generator\n",
    "    noise = torch.randn(64, latent_dim)\n",
    "    fake_data = generator(noise)\n",
    "    fake_output = discriminator(fake_data)\n",
    "    loss_g = criterion(fake_output, real_labels)  # Aim for the generator to produce data that fools the discriminator\n",
    "\n",
    "    # Backpropagate and optimize generator\n",
    "    optimizer_g.zero_grad()\n",
    "    loss_g.backward()\n",
    "    optimizer_g.step()\n",
    "\n",
    "    # Print Losses\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034cbe9",
   "metadata": {},
   "source": [
    "**Explanation of the Code** <br>\n",
    "- Generator: It takes a random noise vector (latent space) as input and outputs a data sample.\n",
    "- Discriminator: It takes a data sample and outputs a probability indicating whether the sample is real (1) or fake (0).\n",
    "- Training Loop:\n",
    "    - Discriminator Training: We train the discriminator on both real data (labelled as 1) and fake data generated by the generator (labelled as 0). This helps the discriminator to learn to distinguish between real and fake samples.\n",
    "    - Generator Training: We train the generator to produce samples that the discriminator misclassifies as real, effectively \"fooling\" the discriminator.\n",
    "\n",
    "**Notes**\n",
    "- Epochs: More epochs might be required to achieve realistic data.\n",
    "- Tuning: Adjusting the architecture, learning rate, and other hyperparameters is key in practical GANs.\n",
    "\n",
    "This example demonstrates the basic setup of a GAN for generating 1D data and can be expanded for more complex data, such as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1b96d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
