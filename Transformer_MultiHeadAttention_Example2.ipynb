{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f521a7e",
   "metadata": {},
   "source": [
    "# Lecture 4 Self Attention - Multi Head Attention\n",
    "## Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76b74e",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bce31",
   "metadata": {},
   "source": [
    "Multi-head attention is a fundamental component of transformer architectures, allowing models to focus on different parts of the input simultaneously. It enhances the model's ability to capture various relationships and dependencies within the data.\n",
    "\n",
    "Below are two Python examples demonstrating how multi-head attention works:\n",
    "- From Scratch Implementation Using NumPy\n",
    "- Using PyTorch's Built-in Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f311f9",
   "metadata": {},
   "source": [
    "#### 2. Multi-Head Attention Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4da451",
   "metadata": {},
   "source": [
    "Leveraging PyTorch's built-in nn.MultiheadAttention module simplifies the implementation, handling much of the complexity internally. This example demonstrates how to use this module within a simple transformer-like setup.\n",
    "\n",
    "**Step-by-Step Explanation**\n",
    "- Import Libraries and Set Parameters:\n",
    "    - Define dimensions, number of heads, and other necessary parameters.\n",
    "- Initialize the MultiheadAttention Module:\n",
    "    - Create an instance of nn.MultiheadAttention with the specified embedding dimension and number of heads.\n",
    "- Prepare Input Data:\n",
    "    - Torch expects input shapes as (seq_length, batch_size, embedding_dim).\n",
    "    - Initialize random input tensors for queries, keys, and values.\n",
    "- Perform Multi-Head Attention:\n",
    "    - Pass the queries, keys, and values to the forward method of the MultiheadAttention module.\n",
    "    - Obtain the output and attention weights.\n",
    "- Output the Results:\n",
    "    - Display the attention output and the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a136878e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X:\n",
      " tensor([[[0.6814, 0.3330, 0.3603, 0.6477, 0.9110, 0.6359, 0.2634, 0.2650],\n",
      "         [0.0273, 0.6080, 0.2194, 0.0542, 0.9384, 0.1753, 0.4431, 0.6432],\n",
      "         [0.5159, 0.1636, 0.0958, 0.8985, 0.5814, 0.9148, 0.3324, 0.6473],\n",
      "         [0.3857, 0.4778, 0.1955, 0.6691, 0.6581, 0.4897, 0.3875, 0.1918]]])\n",
      "\n",
      "Attention Output:\n",
      " tensor([[[ 0.0371, -0.0003,  0.0308, -0.0156,  0.0638, -0.0011, -0.0370,\n",
      "           0.0329],\n",
      "         [ 0.0410, -0.0081,  0.0399, -0.0159,  0.0690, -0.0020, -0.0392,\n",
      "           0.0409],\n",
      "         [ 0.0385, -0.0004,  0.0322, -0.0143,  0.0650,  0.0003, -0.0388,\n",
      "           0.0346],\n",
      "         [ 0.0398, -0.0007,  0.0333, -0.0137,  0.0671,  0.0011, -0.0399,\n",
      "           0.0359]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Attention Weights:\n",
      " tensor([[[0.2563, 0.2470, 0.2437, 0.2530],\n",
      "         [0.2554, 0.2512, 0.2436, 0.2498],\n",
      "         [0.2536, 0.2448, 0.2509, 0.2507],\n",
      "         [0.2554, 0.2424, 0.2496, 0.2525]]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters\n",
    "torch.manual_seed(0)\n",
    "batch_size = 1\n",
    "seq_length = 4\n",
    "embedding_dim = 8\n",
    "num_heads = 2\n",
    "\n",
    "# Initialize MultiheadAttention\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "# Random input\n",
    "X = torch.rand(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "# In PyTorch's MultiheadAttention, inputs are of shape (batch_size, seq_length, embedding_dim) if batch_first=True\n",
    "# Otherwise, shape should be (seq_length, batch_size, embedding_dim)\n",
    "\n",
    "# Perform Multi-Head Attention\n",
    "# Here, we use the same tensor for queries, keys, and values (self-attention)\n",
    "attn_output, attn_weights = multihead_attn(X, X, X)\n",
    "\n",
    "print(\"Input X:\\n\", X)\n",
    "print(\"\\nAttention Output:\\n\", attn_output)\n",
    "print(\"\\nAttention Weights:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595db678",
   "metadata": {},
   "source": [
    "**Output Explanation**\n",
    "\n",
    "- Input X: Randomly generated input tensor with shape (batch_size, seq_length, embedding_dim).\n",
    "- Attention Output: The result of the multi-head attention operation.\n",
    "- Attention Weights: The attention scores for each head, showing how much each position attends to others.\n",
    "\n",
    "Note: The actual numerical values may vary due to random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65765299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
