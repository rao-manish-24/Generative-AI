{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3cb2787",
   "metadata": {},
   "source": [
    "# Lecture 5 Transformer - Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0229dd2",
   "metadata": {},
   "source": [
    "Layer Normalization is typically applied to the output of each sub-layer in the encoder. Here's a breakdown of the code:\n",
    "\n",
    "Layer Normalization normalizes the input across the features for each data point (rather than across the batch, as in Batch Normalization). It subtracts the mean and divides by the standard deviation, followed by scaling with a learned weight (gamma) and shifting with a learned bias (beta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2cf96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[0.2 0.8 1.  1.2]\n",
      " [1.  0.  0.5 1.5]]\n",
      "\n",
      " Output after Layer Normalization:\n",
      "[[-1.60356317  0.          0.53452106  1.06904211]\n",
      " [ 0.4472128  -1.34163839 -0.4472128   1.34163839]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize Layer Normalization parameters.\n",
    "        \n",
    "        Args:\n",
    "            d_model: The dimension of the model (number of features).\n",
    "            eps: A small constant for numerical stability (to avoid division by zero).\n",
    "        \"\"\"\n",
    "        self.gamma = np.ones((d_model,))\n",
    "        self.beta = np.zeros((d_model,))\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Apply layer normalization to the input x.\n",
    "        \n",
    "        Args:\n",
    "            x: Input matrix of shape (batch_size, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Normalized input with the same shape as x.\n",
    "        \"\"\"\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        std = np.std(x, axis=-1, keepdims=True)\n",
    "        \"\"\"\n",
    "        axis=-1 refers to the last axis of the array, \n",
    "        which is the d_model dimension (number of features) in this case. \n",
    "        Thus, the mean and std are calculated along the d_model dimension, \n",
    "        which means: For each batch (each row in x), \n",
    "        the mean and standard deviation are calculated across the features (across the elements of that row).\n",
    "        \"\"\" \n",
    "        x_normalized = (x - mean) / (std + self.eps)\n",
    "        return self.gamma * x_normalized + self.beta\n",
    "\n",
    "# Example usage of LayerNorm in a Transformer Encoder\n",
    "\n",
    "class TransformerEncoderLayer:\n",
    "    def __init__(self, d_model):\n",
    "        \"\"\"\n",
    "        A simplified Transformer Encoder layer with layer normalization.\n",
    "        \n",
    "        Args:\n",
    "            d_model: The dimension of the model.\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.layer_norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Input matrix of shape (batch_size, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Output matrix of shape (batch_size, d_model)\n",
    "        \"\"\"\n",
    "        # Here, we can have sub-layers like self-attention and feedforward (not implemented)\n",
    "        \n",
    "        # Apply layer normalization (after sub-layers processing, for simplicity assume x)\n",
    "        x_norm = self.layer_norm(x)\n",
    "        \n",
    "        return x_norm\n",
    "\n",
    "# Testing the encoder layer with layer normalization\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample input (batch_size = 2, d_model = 4)\n",
    "    x = np.array([[0.2, 0.8, 1.0, 1.2],\n",
    "                  [1.0, 0.0, 0.5, 1.5]])\n",
    "\n",
    "    encoder_layer = TransformerEncoderLayer(d_model=4)\n",
    "    output = encoder_layer.forward(x)\n",
    "\n",
    "    print(\"Input:\")\n",
    "    print(x)\n",
    "    print(\"\\n Output after Layer Normalization:\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7775bd",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "1. LayerNorm class: Implements layer normalization, using the formula:<br>\n",
    "$LayerNorm(x) = \\gamma \\frac{x - \\mu}{\\sigma + \\epsilon}+\\beta$\n",
    "\n",
    "where:<br>\n",
    "- $\\mu$ is the mean of the input.\n",
    "- $\\sigma$ is the standard deviation.\n",
    "- $\\epsilon$ is a small number to avoid division by zero.\n",
    "- $\\gamma$ and $\\beta$ are set to be $\\gamma = 1$ and $\\beta = 0$ by default. They can also be learned parameters.\n",
    "\n",
    "2. TransformerEncoderLayer class: Represents a simplified encoder layer in the Transformer. Normally, it would have sub-layers like self-attention and feed-forward networks, but here we focus on layer normalization.\n",
    "\n",
    "3. Output: The code outputs the normalized input using layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ded2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
