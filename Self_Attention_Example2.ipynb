{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85648aa5",
   "metadata": {},
   "source": [
    "# Lecture 4 Self-Attention\n",
    "\n",
    "#### 2 Self-Attention Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e403426",
   "metadata": {},
   "source": [
    "Leveraging PyTorch, this example implements self-attention as a module, showcasing how it can be integrated into neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f29ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18b7e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, heads = 1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        assert embed_dim % heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_dim // heads\n",
    "\n",
    "        # Define linear layers for queries, keys, and values\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Output linear layer\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.query(x).view(batch_size, seq_length, self.heads, self.head_dim)\n",
    "        K = self.key(x).view(batch_size, seq_length, self.heads, self.head_dim)\n",
    "        V = self.value(x).view(batch_size, seq_length, self.heads, self.head_dim)\n",
    "\n",
    "        # Transpose for attention calculation\n",
    "        Q = Q.transpose(1, 2)  # (batch, heads, seq_length, head_dim)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        out = torch.matmul(attention, V)  # (batch, heads, seq_length, head_dim)\n",
    "\n",
    "        # Concatenate heads\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_dim)\n",
    "\n",
    "        # Final linear layer\n",
    "        out = self.out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a5ef4",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe620d92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences: [['cash', 'is', 'the', 'king'], ['do', 'not', 'time', 'the', 'market']]\n",
      "Vocabulary: {'cash': 1, 'is': 2, 'the': 3, 'king': 4, 'do': 5, 'not': 6, 'time': 7, 'market': 8}\n",
      "Encoded Sentences: [[1, 2, 3, 4], [5, 6, 7, 3, 8]]\n",
      "Padded Sentences: [[1, 2, 3, 4, 0], [5, 6, 7, 3, 8]]\n",
      "Max Length: 5\n",
      "Encoded Tensor:\n",
      "tensor([[1, 2, 3, 4, 0],\n",
      "        [5, 6, 7, 3, 8]])\n",
      "Tensor Shape: torch.Size([2, 5])\n",
      "One-Hot Encoded Tensor:\n",
      "tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n",
      "One-Hot Tensor Shape: torch.Size([2, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Define your sentences\n",
    "sentences = [ \n",
    "    \"Cash is the king\",\n",
    "    \"Do not time the market\",\n",
    "]\n",
    "\n",
    "# 2. Tokenize the sentences\n",
    "# Convert to lowercase and split by spaces\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "print(\"Tokenized Sentences:\", tokenized_sentences)\n",
    "\n",
    "# 3. Build a vocabulary\n",
    "# Assign a unique index to each unique token\n",
    "# Reserve 0 for padding\n",
    "vocab = {}\n",
    "current_index = 1  # Start indexing from 1\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for token in sentence:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Optionally, add an <UNK> token for unknown words\n",
    "# vocab['<UNK>'] = current_index\n",
    "# current_index += 1\n",
    "\n",
    "# 4. Encode the sentences\n",
    "# Replace each token with its corresponding index\n",
    "encoded_sentences = []\n",
    "for sentence in tokenized_sentences:\n",
    "    encoded = [vocab.get(token, 0) for token in sentence]  # Use 0 if token not found\n",
    "    encoded_sentences.append(encoded)\n",
    "\n",
    "print(\"Encoded Sentences:\", encoded_sentences)\n",
    "\n",
    "# 5. Pad the sequences\n",
    "# Find the length of the longest sentence\n",
    "max_length = max(len(sentence) for sentence in encoded_sentences)\n",
    "\n",
    "# Pad shorter sentences with 0 (padding index)\n",
    "padded_sentences = [\n",
    "    sentence + [0] * (max_length - len(sentence)) for sentence in encoded_sentences\n",
    "]\n",
    "\n",
    "print(\"Padded Sentences:\", padded_sentences)\n",
    "\n",
    "# Verify max_length is 5\n",
    "print(\"Max Length:\", max_length)\n",
    "\n",
    "# 6. Convert to PyTorch tensor\n",
    "encoded_tensor = torch.tensor(padded_sentences, dtype=torch.long)\n",
    "print(\"Encoded Tensor:\")\n",
    "print(encoded_tensor)\n",
    "print(\"Tensor Shape:\", encoded_tensor.shape)  # Should be (2, 5)\n",
    "\n",
    "# 7. One-Hot Encode the tensor\n",
    "# Number of classes is the size of the vocabulary + 1 for padding (if not already included)\n",
    "num_classes = len(vocab) + 1  # +1 for padding index 0\n",
    "\n",
    "# Use F.one_hot to convert to one-hot vectors\n",
    "# F.one_hot expects the class indices to be in the last dimension\n",
    "one_hot_tensor = F.one_hot(encoded_tensor, num_classes=num_classes).float()\n",
    "\n",
    "print(\"One-Hot Encoded Tensor:\")\n",
    "print(one_hot_tensor)\n",
    "print(\"One-Hot Tensor Shape:\", one_hot_tensor.shape)  # Should be (2, 5, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c7f41",
   "metadata": {},
   "source": [
    "**Explanation of Each Step:**\n",
    "- Tokenization:\n",
    "    - Converts each sentence to lowercase for consistency.\n",
    "    - Splits sentences into lists of tokens (words).\n",
    "- Vocabulary Building:\n",
    "    - Iterates through all tokens and assigns a unique integer to each unique token.\n",
    "    - Starts indexing from 1 to reserve 0 for padding purposes.\n",
    "- Encoding:\n",
    "    - Transforms each token in the sentences to its corresponding integer index based on the vocabulary.\n",
    "    - Tokens not found in the vocabulary are replaced with 0 (you can use an UNK token for unknown words if desired).\n",
    "- Padding:\n",
    "    - Determines the length of the longest sentence to ensure all sequences are of equal length.\n",
    "    - Pads shorter sentences with 0 (the padding index).\n",
    "- One-Hot Encoding:\n",
    "    - Converts the indexed tokens into one-hot vectors.\n",
    "    - The torch.nn.functional.one_hot function is used for this purpose.\n",
    "- The initial one-hot encoding includes the padding class, resulting in a tensor of shape (2, 5, 9). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9d4d6df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Output:\n",
      " tensor([[[-0.0061, -0.0348,  0.0347, -0.4062,  0.1152,  0.0412,  0.1987,\n",
      "          -0.0271, -0.2182],\n",
      "         [-0.0051, -0.0353,  0.0381, -0.4055,  0.1155,  0.0411,  0.2021,\n",
      "          -0.0265, -0.2188],\n",
      "         [-0.0052, -0.0356,  0.0354, -0.4043,  0.1130,  0.0436,  0.1982,\n",
      "          -0.0231, -0.2155],\n",
      "         [-0.0061, -0.0367,  0.0412, -0.4041,  0.1131,  0.0418,  0.2036,\n",
      "          -0.0237, -0.2201],\n",
      "         [-0.0051, -0.0361,  0.0401, -0.4041,  0.1133,  0.0435,  0.2011,\n",
      "          -0.0236, -0.2168]],\n",
      "\n",
      "        [[ 0.0310, -0.0283, -0.0594, -0.3845,  0.0206,  0.0767,  0.1619,\n",
      "          -0.0365, -0.2167],\n",
      "         [ 0.0311, -0.0294, -0.0583, -0.3841,  0.0170,  0.0780,  0.1629,\n",
      "          -0.0351, -0.2180],\n",
      "         [ 0.0305, -0.0271, -0.0605, -0.3848,  0.0251,  0.0758,  0.1612,\n",
      "          -0.0379, -0.2142],\n",
      "         [ 0.0309, -0.0283, -0.0597, -0.3843,  0.0205,  0.0766,  0.1618,\n",
      "          -0.0364, -0.2166],\n",
      "         [ 0.0305, -0.0279, -0.0596, -0.3843,  0.0222,  0.0761,  0.1616,\n",
      "          -0.0372, -0.2161]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "self_attention = SelfAttention(embed_dim = 9, heads = 1)\n",
    "output = self_attention(one_hot_tensor)\n",
    "print(\"Self-Attention Output:\\n\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
