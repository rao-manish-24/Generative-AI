{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a853ca0c",
   "metadata": {},
   "source": [
    "# RNN Example 1: Toy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ef7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b12dfbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 2\n",
    "hidden_size = 3\n",
    "hidden_state_prev = np.zeros((hidden_size,1))  \n",
    "inputs = [1,0,1,0]\n",
    "targets = [0,1,0,1]\n",
    "\n",
    "# Initialize weights and biases (parameters)\n",
    "# parameter U: x(t) -> h(t), 3 x 2\n",
    "input_weights_U = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "# parameter W: x(t) -> h(t), 3 x 3\n",
    "hidden_weights_W = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "# parameter b: a(t)=b+W h(t-1)+Ux(t) and h(t) = tahn(a(t)), 3 x 1\n",
    "hidden_bias = np.zeros((hidden_size, 1)) \n",
    "# parameter V: y(t)= c + Vh(t) 2 x 3\n",
    "output_weights_V = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "# parameter c: y(t)= c + Vh(t) 2 x 1\n",
    "output_bias = np.zeros((vocab_size, 1))\n",
    "\n",
    "# Forward pass: choose dictionaries as data type with keys to be the timestamp. \n",
    "xs, target, hidden_states, outputs, probabilities = {}, {}, {}, {}, {}\n",
    "loss = 0\n",
    "hidden_states[-1] = np.copy(hidden_state_prev)\n",
    "for t in range(len(inputs)): \n",
    "    # one-hot-encoding the input character \n",
    "    xs[t] = np.zeros((vocab_size,1))  # 2 x 1\n",
    "    character = inputs[t] # the first element in the input is 1\n",
    "    xs[t][character] = 1\n",
    "    target[t] = np.zeros((vocab_size,1))  # 2 x 1\n",
    "    target_character = targets[t]\n",
    "    target[t][target_character] = 1\n",
    "    # Compute hidden state (3 x 2)@(2 x 1) + (3 x 3)@(3 x 1)\n",
    "    hidden_states[t] = np.tanh(input_weights_U @ xs[t] + hidden_weights_W @ hidden_states[t-1] + hidden_bias)\n",
    "    # Compute output and probabilities (2 x 3)@(3 x 3) = (2 x 1)\n",
    "    outputs[t] = output_weights_V @ hidden_states[t] + output_bias\n",
    "    probabilities[t] = np.exp(outputs[t]) / np.sum(np.exp(outputs[t]))\n",
    "    # Compute cross-entropy loss\n",
    "    loss += - sum(target[t] * np.log(probabilities[t]))\n",
    "    # Note: target swiches bwtween the 1st or the 2nd row, 0 refers to the first element in the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9760d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.77221943]\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073e82d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
