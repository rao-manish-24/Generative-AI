{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f521a7e",
   "metadata": {},
   "source": [
    "# Lecture 4 Self Attention - Masked Attention\n",
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76b74e",
   "metadata": {},
   "source": [
    "#### Masked Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bce31",
   "metadata": {},
   "source": [
    "Masked attention is a crucial mechanism in transformer architectures, particularly in tasks like language modeling, where it's essential to prevent the model from accessing future tokens during training. This ensures that the predictions for a particular position depend only on the known outputs at positions before it.\n",
    "\n",
    "Below are two Python examples demonstrating masked attention:\n",
    "- From Scratch Implementation Using NumPy\n",
    "- Using PyTorch's Built-in Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d62b20",
   "metadata": {},
   "source": [
    "#### 1. Masked Attention from Scratch Using NumPy\n",
    "This example illustrates how to implement masked attention manually using NumPy. It demonstrates how to prevent each position in the sequence from attending to future positions by applying a mask to the attention scores.\n",
    "\n",
    "Step-by-Step Explanation\n",
    "- Initialize Parameters: \n",
    "    - Define dimensions for embedding, queries, keys, values, and the sequence length.\n",
    "    - Initialize random weights for linear transformations.\n",
    "- Create Query, Key, and Value Matrices:\n",
    "    - Transform the input data into queries (Q), keys (K), and values (V) using the initialized weights.\n",
    "- Compute Scaled Dot-Product Attention with Masking:\n",
    "    - Calculate attention scores using the dot product of Q and K.\n",
    "    - Apply a mask to prevent attention to future positions by setting those scores to a large negative value.\n",
    "    - Scale the scores, apply softmax, and multiply by V to obtain the context.\n",
    "- Final Linear Transformation:\n",
    "    - Apply a final linear transformation to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ab4c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X:\n",
      " [[[0.3745 0.9507 0.732  0.5987 0.156  0.156  0.0581 0.8662]\n",
      "  [0.6011 0.7081 0.0206 0.9699 0.8324 0.2123 0.1818 0.1834]\n",
      "  [0.3042 0.5248 0.4319 0.2912 0.6119 0.1395 0.2921 0.3664]\n",
      "  [0.4561 0.7852 0.1997 0.5142 0.5924 0.0465 0.6075 0.1705]]]\n",
      "\n",
      "Attention Weights:\n",
      " [[[1.     0.     0.     0.    ]\n",
      "  [0.7894 0.2106 0.     0.    ]\n",
      "  [0.7407 0.1907 0.0686 0.    ]\n",
      "  [0.6808 0.182  0.0573 0.0799]]]\n",
      "\n",
      "Output after Masked Attention:\n",
      " [[[12.0616 10.0441  8.4291  6.0198  7.4703  7.4587  7.5161  9.2008]\n",
      "  [12.0264  9.9897  8.401   6.0266  7.5096  7.3866  7.5354  9.1647]\n",
      "  [11.8611  9.8539  8.293   5.9457  7.4273  7.2924  7.443   9.0455]\n",
      "  [11.7684  9.7834  8.2206  5.9009  7.3752  7.2382  7.3845  8.977 ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "# Parameters\n",
    "np.random.seed(42)  # For reproducibility\n",
    "batch_size = 1\n",
    "seq_length = 4\n",
    "embedding_dim = 8\n",
    "\n",
    "# Random input\n",
    "X = np.random.rand(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "# Initialize weights\n",
    "W_q = np.random.rand(embedding_dim, embedding_dim)\n",
    "W_k = np.random.rand(embedding_dim, embedding_dim)\n",
    "W_v = np.random.rand(embedding_dim, embedding_dim)\n",
    "W_o = np.random.rand(embedding_dim, embedding_dim)\n",
    "\n",
    "# Linear projections\n",
    "Q = X @ W_q  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "K = X @ W_k\n",
    "V = X @ W_v\n",
    "\n",
    "# Scaled Dot-Product Attention with Masking\n",
    "scores = Q @ K.transpose(0, 2, 1) / np.sqrt(embedding_dim)  # (batch_size, seq_length, seq_length)\n",
    "\n",
    "# Create mask: prevent attention to future positions\n",
    "mask = np.triu(np.ones((seq_length, seq_length)), k=1)  # Upper triangular matrix with zeros on and below the diagonal\n",
    "scores = scores - mask * 1e9  # Large negative value to mask\n",
    "\n",
    "# Apply softmax\n",
    "attention_weights = softmax(scores, axis=-1)  # (batch_size, seq_length, seq_length)\n",
    "\n",
    "# Multiply by V\n",
    "context = attention_weights @ V  # (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "# Final linear layer\n",
    "output = context @ W_o  # (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "# Display Results\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(\"Input X:\\n\", X)\n",
    "print(\"\\nAttention Weights:\\n\", attention_weights)\n",
    "print(\"\\nOutput after Masked Attention:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359fe05c",
   "metadata": {},
   "source": [
    "**Explanation of Output**\n",
    "- Input X: A randomly generated input tensor with shape (batch_size, seq_length, embedding_dim).\n",
    "- Attention Weights: After applying the mask, each position can only attend to itself and previous positions. For example, the first position attends only to itself, the second attends to the first and second, and so on.\n",
    "- Output: The result of the masked attention mechanism, maintaining the same shape as the input.\n",
    "\n",
    "Note: The actual numerical values may vary slightly due to random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b547af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
