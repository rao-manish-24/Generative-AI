{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4add387b",
   "metadata": {},
   "source": [
    "# Lecture 04 Self-Attention\n",
    "\n",
    "#### 1 Basic Self-Attention Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26284a",
   "metadata": {},
   "source": [
    "This example demonstrates the core concept of self-attention without relying on any deep learning libraries. It computes the attention scores and outputs for a simple input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908f6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True) \n",
    "    # probabilities along the row equal to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8568df",
   "metadata": {},
   "source": [
    "1. Subtracting the Maximum Value\n",
    "    - Purpose: Improve numerical stability to prevent potential overflow issues when computing exponentials.\n",
    "\n",
    "2. The axis parameter ```axis=-1``` specifies the dimension along which to perform the operation. In NumPy, negative integers can be used to refer to axes from the end. Here, axis=-1 refers to the last axis of the array. For example:\n",
    "    - If x is a 2D array (matrix) with shape (m, n), axis=-1 is equivalent to axis=1, which operates along the columns.\n",
    "    - If x is a 3D array with shape (a, b, c), axis=-1 refers to the c dimension. \n",
    "\n",
    "3. The last line ```e_x / e_x.sum(...)``` divides each exponential by the sum of exponentials along the specified axis (this this case, along the row), resulting in probabilities that sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b56fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_basic(inputs):\n",
    "    \"\"\"\n",
    "    Compute self-attention for input vectors.\n",
    "\n",
    "    Args:\n",
    "        inputs: A NumPy array of shape (sequence_length, embedding_dim)\n",
    "    \n",
    "    Returns:\n",
    "        output: Self-attended output of shape (sequence_length, embedding_dim)\n",
    "    \"\"\"\n",
    "    # Initialize weight matrices (for simplicity, using identity matrices)\n",
    "    # inputs.shape[1] is embed_dim, i.e. d_model \n",
    "    W_q = np.eye(inputs.shape[1])\n",
    "    W_k = np.eye(inputs.shape[1])\n",
    "    W_v = np.eye(inputs.shape[1])\n",
    "\n",
    "    # Compute queries, keys, and values\n",
    "    Q = inputs @ W_q  # (seq_len x d_model) x (d_model x d_model), d_model is embed_dim\n",
    "    print(\"The query Q is:\\n\", Q)\n",
    "    K = inputs @ W_k  # (seq_len x d_model)\n",
    "    print(\"The key K is:\\n\", K)\n",
    "    V = inputs @ W_v  # (seq_len x d_model)\n",
    "    print(\"The value (meaning) V is:\\n\", V)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = Q @ K.T # (seq_len, seq_len)\n",
    "    print(\"The attention score is:\\n\", scores)\n",
    "    # Scaled Dot-Product Attention: scores = np.dot(Q, K.T) / np.sqrt(inputs.shape[1])  # (seq_len, seq_len)\n",
    "    attention_weights = softmax(scores)  # (seq_len, seq_len)\n",
    "    print(\"The attention weights to be applied on value (meaning) is:\\n\", attention_weights)\n",
    "    \n",
    "    # Compute the weighted sum of values\n",
    "    output = attention_weights @ V  # (seq_len, embed_dim)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca95975",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2514eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query Q is:\n",
      " [[1. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "The key K is:\n",
      " [[1. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "The value (meaning) V is:\n",
      " [[1. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "The attention score is:\n",
      " [[2. 0. 2.]\n",
      " [0. 2. 2.]\n",
      " [2. 2. 4.]]\n",
      "The attention weights to be applied on value (meaning) is:\n",
      " [[0.46831053 0.06337894 0.46831053]\n",
      " [0.06337894 0.46831053 0.46831053]\n",
      " [0.10650698 0.10650698 0.78698604]]\n",
      "Self-Attention Output:\n",
      " [[0.93662106 0.53168947 0.93662106 0.53168947]\n",
      " [0.53168947 0.93662106 0.53168947 0.93662106]\n",
      " [0.89349302 0.89349302 0.89349302 0.89349302]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Sample input: 3 tokens with embedding size 4\n",
    "    inputs = np.array([\n",
    "        [1, 0, 1, 0],\n",
    "        [0, 1, 0, 1],\n",
    "        [1, 1, 1, 1]\n",
    "    ])\n",
    "\n",
    "    output = self_attention_basic(inputs)\n",
    "    np.set_printoptions(suppress=True, precision=8)\n",
    "    print(\"Self-Attention Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c6e606",
   "metadata": {},
   "source": [
    "**Explanation:** <br>\n",
    "- Input Representation: The input is a sequence of vectors (e.g., word embeddings). In this example, we have a sequence length of 3 with embedding dimensions of 4.\n",
    "- Weight Matrices: For simplicity, the weight matrices W_q, W_k, and W_v (used to compute queries, keys, and values) are initialized as identity matrices. In practice, these are learned during training.\n",
    "- Queries, Keys, and Values: Compute queries (Q), keys (K), and values (V) by multiplying the input with the respective weight matrices.\n",
    "- Attention Scores: Calculate the attention scores by taking the dot product of Q and the transpose of K (in Transformer, you need to scale the product by the square root of the embedding dimension to stabilize gradients.)\n",
    "- Softmax: Apply the softmax function to obtain attention weights, which indicate the importance of each token in the sequence relative to others.\n",
    "- Weighted Sum: Multiply the attention weights with the values V to get the final self-attended output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1aab35",
   "metadata": {},
   "source": [
    "#### Example 2 Jiraiya Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc079b36",
   "metadata": {},
   "source": [
    "<img src=\"Jiraiya_Code.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9dd702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query Q is:\n",
      " [[  9.  31.   8.]\n",
      " [106.   7.   0.]\n",
      " [207.  15.   0.]]\n",
      "The key K is:\n",
      " [[  9.  31.   8.]\n",
      " [106.   7.   0.]\n",
      " [207.  15.   0.]]\n",
      "The value (meaning) V is:\n",
      " [[  9.  31.   8.]\n",
      " [106.   7.   0.]\n",
      " [207.  15.   0.]]\n",
      "The attention score is:\n",
      " [[ 1106.  1171.  2328.]\n",
      " [ 1171. 11285. 22047.]\n",
      " [ 2328. 22047. 43074.]]\n",
      "The attention weights to be applied on value (meaning) is:\n",
      " [[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "Self-Attention Output:\n",
      " [[207.  15.   0.]\n",
      " [207.  15.   0.]\n",
      " [207.  15.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example 2 Jiraiya Code\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample input: 3 tokens with embedding size 3\n",
    "    inputs = np.array([\n",
    "        [9, 31, 8],\n",
    "        [106, 7, 0],\n",
    "        [207, 15, 0]\n",
    "    ])\n",
    "\n",
    "    output = self_attention_basic(inputs)\n",
    "    np.set_printoptions(suppress=True, precision=8)\n",
    "    print(\"Self-Attention Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aaba840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To improve numerical stability, x - np.max(x) is:  [-1222 -1157     0]\n",
      "The softmax score is:  [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "x = [1106,  1171,  2328]\n",
    "print('To improve numerical stability, x - np.max(x) is: ', x - np.max(x, axis=-1, keepdims=True))\n",
    "e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "print('The softmax score is: ', e_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9850f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
