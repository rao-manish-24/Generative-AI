{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f521a7e",
   "metadata": {},
   "source": [
    "# Lecture 4 Self Attention - Multi Head Attention\n",
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76b74e",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bce31",
   "metadata": {},
   "source": [
    "Multi-head attention is a fundamental component of transformer architectures, allowing models to focus on different parts of the input simultaneously. It enhances the model's ability to capture various relationships and dependencies within the data.\n",
    "\n",
    "Below are two Python examples demonstrating how multi-head attention works:\n",
    "- From Scratch Implementation Using NumPy\n",
    "- Using PyTorch's Built-in Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbdf6e6",
   "metadata": {},
   "source": [
    "#### 1. Multi-Head Attention from Scratch Using NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e42d0",
   "metadata": {},
   "source": [
    "This example illustrates the core mechanics of multi-head attention without relying on deep learning libraries. It includes the creation of query, key, and value matrices, splitting them into multiple heads, performing scaled dot-product attention, and concatenating the results.\n",
    "\n",
    "Step-by-Step Explanation\n",
    "- Initialize Parameters: \n",
    "    - Define dimensions for embedding, queries, keys, values, and the number of heads.\n",
    "    - Initialize random weights for linear transformations.\n",
    "\n",
    "- Create Query, Key, and Value Matrices:\n",
    "    - Input data is transformed into queries (Q), keys (K), and values (V) using the initialized weights.\n",
    "\n",
    "- Split into Multiple Heads:\n",
    "    - Q, K, and V are split into multiple heads to allow the model to attend to information from different representation subspaces.\n",
    "\n",
    "- Scaled Dot-Product Attention:\n",
    "    - For each head, compute attention scores using the dot product of Q and K, scale them, apply softmax, and multiply by V.\n",
    "\n",
    "- Concatenate Heads and Final Linear Transformation:\n",
    "    - Concatenate the outputs from all heads and apply a final linear transformation to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc363e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X:\n",
      " [[[0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
      "   0.43758721 0.891773  ]\n",
      "  [0.96366276 0.38344152 0.79172504 0.52889492 0.56804456 0.92559664\n",
      "   0.07103606 0.0871293 ]\n",
      "  [0.0202184  0.83261985 0.77815675 0.87001215 0.97861834 0.79915856\n",
      "   0.46147936 0.78052918]\n",
      "  [0.11827443 0.63992102 0.14335329 0.94466892 0.52184832 0.41466194\n",
      "   0.26455561 0.77423369]]]\n",
      "\n",
      "Output after Multi-Head Attention:\n",
      " [[[ 9.19301463 10.44328382  9.2244454   8.05737673 10.98670376\n",
      "    9.43520132 10.65160547  9.78990228]\n",
      "  [ 9.10985062 10.36255368  9.14890231  7.99563435 10.88414448\n",
      "    9.35370385 10.56035521  9.70807359]\n",
      "  [ 9.21809014 10.45357218  9.24102286  8.0718153  11.01227309\n",
      "    9.45719822 10.66877882  9.80798268]\n",
      "  [ 9.05051238 10.29643008  9.09708768  7.94442927 10.80930673\n",
      "    9.29190295 10.48942527  9.64204537]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "# Parameters\n",
    "np.random.seed(0)\n",
    "batch_size = 1\n",
    "seq_length = 4\n",
    "embedding_dim = 8\n",
    "num_heads = 2\n",
    "head_dim = embedding_dim // num_heads\n",
    "\n",
    "# Random input\n",
    "X = np.random.rand(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "# Initialize weights\n",
    "W_q = np.random.rand(embedding_dim, embedding_dim)\n",
    "W_k = np.random.rand(embedding_dim, embedding_dim)\n",
    "W_v = np.random.rand(embedding_dim, embedding_dim)\n",
    "W_o = np.random.rand(embedding_dim, embedding_dim)\n",
    "\n",
    "# Linear projections\n",
    "Q = X @ W_q  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "K = X @ W_k\n",
    "V = X @ W_v\n",
    "\n",
    "# Split into heads\n",
    "def split_heads(x, num_heads, head_dim):\n",
    "    batch_size, seq_length, embed_dim = x.shape\n",
    "    x = x.reshape(batch_size, seq_length, num_heads, head_dim)\n",
    "    return x.transpose(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "Q = split_heads(Q, num_heads, head_dim)\n",
    "K = split_heads(K, num_heads, head_dim)\n",
    "V = split_heads(V, num_heads, head_dim)\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(head_dim)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "attention = softmax(scores, axis=-1)\n",
    "context = attention @ V  # (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "# Concatenate heads\n",
    "context = context.transpose(0, 2, 1, 3).reshape(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "# Final linear layer\n",
    "output = context @ W_o  # (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "print(\"Input X:\\n\", X)\n",
    "print(\"\\nOutput after Multi-Head Attention:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a747d",
   "metadata": {},
   "source": [
    "**Output Explanation**\n",
    "- Input X: Randomly generated input tensor with shape (batch_size, seq_length, embedding_dim).\n",
    "- Output: The result of the multi-head attention mechanism, maintaining the same shape as the input.\n",
    "\n",
    "Note: The actual numerical values may vary due to random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643dc654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
