{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8616e69f",
   "metadata": {},
   "source": [
    "# Lecture 4 Self-Attention\n",
    "\n",
    "#### 3. Self-Attention in a Simple Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d03aef",
   "metadata": {},
   "source": [
    "This example integrates self-attention into a miniature Transformer model for a simple task, such as language modeling or sequence classification. We'll use PyTorch's built-in nn.MultiheadAttention for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75e3d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408453f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_classes):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 100, embed_dim))  # Max seq length=100\n",
    "\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Output tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        embed = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        embed = embed.transpose(0, 1)  # (seq_length, batch_size, embed_dim)\n",
    "\n",
    "        # Self-Attention\n",
    "        attn_output, _ = self.self_attention(embed, embed, embed)\n",
    "        attn_output = self.layer_norm1(embed + attn_output)\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ff_output = self.feed_forward(attn_output)\n",
    "        ff_output = self.layer_norm2(attn_output + ff_output)\n",
    "\n",
    "        # Pooling (e.g., take the mean)\n",
    "        pooled = ff_output.mean(dim=0)  # (batch_size, embed_dim)\n",
    "\n",
    "        logits = self.classifier(pooled)  # (batch_size, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e883b0",
   "metadata": {},
   "source": [
    "#### Example 1: A Good Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31dec844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " tensor([[ 0.0734, -0.2996,  0.4836],\n",
      "        [-0.0879, -0.4025,  0.5060],\n",
      "        [-0.1356, -0.2478,  0.7741],\n",
      "        [-0.1112,  0.0129,  0.4803]], grad_fn=<AddmmBackward0>)\n",
      "Loss: 1.1979998350143433\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    vocab_size = 50\n",
    "    embed_dim = 16\n",
    "    num_heads = 2\n",
    "    hidden_dim = 32\n",
    "    num_classes = 3\n",
    "    batch_size = 4\n",
    "    seq_length = 10\n",
    "\n",
    "    # Sample input: batch of sequences with token indices\n",
    "    x = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "\n",
    "    # Initialize model, loss, optimizer\n",
    "    model = SimpleTransformer(vocab_size, embed_dim, num_heads, hidden_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(x)\n",
    "    print(\"Logits:\\n\", logits)\n",
    "\n",
    "    # Sample target labels\n",
    "    targets = torch.tensor([0, 2, 1, 0])\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(logits, targets)\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    # Backward pass and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4e5495",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Model Components:\n",
    "    - Embedding Layer: Converts token indices into dense vectors.\n",
    "    - Positional Encoding: Adds positional information to embeddings to retain the order of tokens.\n",
    "    - Multihead Self-Attention: Applies self-attention to allow the model to focus on different parts of the input sequence simultaneously.\n",
    "    - Feed-Forward Network: Processes the output of the attention mechanism through a two-layer neural network with a ReLU activation.\n",
    "    - Layer Normalization: Applies normalization to stabilize and accelerate training.\n",
    "    - Classifier: Maps the processed features to the desired number of output classes.\n",
    "\n",
    "- Forward Pass:\n",
    "    - Embedding & Positional Encoding: Combine embeddings with positional information.\n",
    "    - Self-Attention: Apply multi-head self-attention where the input serves as queries, keys, and values.\n",
    "    - Residual Connections & Layer Norm: Add residual connections followed by layer normalization to facilitate gradient flow.\n",
    "    - Feed-Forward Processing: Pass the attention output through the feed-forward network with another residual connection and normalization.\n",
    "    - Pooling: Aggregate the sequence information by averaging over the sequence length.\n",
    "    - Classification: Produce logits for each class.\n",
    "\n",
    "- Training Step:\n",
    "    - Input Generation: Create random input sequences with token indices.\n",
    "    - Forward Pass: Compute the logits by passing inputs through the model.\n",
    "    - Loss Computation: Calculate cross-entropy loss between predictions and target labels.\n",
    "    - Backward Pass & Optimization: Perform backpropagation and update model parameters using the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddbce1f",
   "metadata": {},
   "source": [
    "**Explanation of Hyperparameters:**\n",
    "- vocab_size: This is usually related to the size of your dictionary of words or tokens. In this case, it could be used if you're working with a model that tokenizes input data (for instance, in NLP tasks).\n",
    "- num_heads: Commonly used in multi-head attention mechanisms, particularly in Transformer models. The num_heads parameter determines how many different \"attention\" heads will operate in parallel.\n",
    "- hidden_dim: The size of the hidden layers in a neural network. If you're using a neural network with multiple layers (like an LSTM or Transformer), this parameter controls how many features each layer has.\n",
    "- num_classes: If your task is a classification task, num_classes defines the number of different possible output labels.\n",
    "- batch_size: This specifies how many samples (rows) are processed at once. Since your DataFrame has 5 rows, the batch_size is 5 here.\n",
    "- seq_length: This refers to the length of each sequence (the number of columns in the DataFrame, in this case, 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8cbd21",
   "metadata": {},
   "source": [
    "#### Example 2: A Bad Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bfac88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " tensor([[0.1998, 0.2814],\n",
      "        [0.2486, 0.3454],\n",
      "        [0.4948, 0.2524],\n",
      "        [0.4785, 0.2527],\n",
      "        [0.4345, 0.2729]], grad_fn=<AddmmBackward0>)\n",
      "Loss: 0.6647613644599915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\yizhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Download stock data\n",
    "start_date = datetime(2024, 9, 30)\n",
    "end_date = datetime(2024, 10, 5)\n",
    "stock_symbol = 'SPY'\n",
    "stocks = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Example DataFrame:\n",
    "#              Open    High     Low   Close  Adj Close    Volume\n",
    "# Date                                                       \n",
    "# 2024-09-30  450.0  455.0  445.0  452.0      452.0  1000000\n",
    "# ... (and so on for 6 days)\n",
    "\n",
    "# Binning the data to fit vocab_size\n",
    "vocab_size = 6  # Define number of bins\n",
    "x = torch.tensor(pd.cut(stocks.values.flatten(), bins=vocab_size, labels=False), dtype=torch.long)\n",
    "x = x.view(stocks.shape)  # Reshape to original DataFrame shape\n",
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 16\n",
    "num_heads = 2\n",
    "hidden_dim = 3\n",
    "num_classes = 2\n",
    "batch_size = 5\n",
    "seq_length = stocks.shape[1]  # Number of features (e.g., Open, High, Low, etc.)\n",
    "\n",
    "# Define SimpleTransformer (Example Implementation)\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_classes):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_length, embed_dim)\n",
    "        embedded = embedded.permute(1, 0, 2)  # Transformer expects (seq_length, batch_size, embed_dim)\n",
    "        transformer_out = self.transformer_encoder(embedded)\n",
    "        transformer_out = transformer_out.mean(dim=0)  # Aggregate over seq_length\n",
    "        logits = self.fc(transformer_out)\n",
    "        return logits\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = SimpleTransformer(vocab_size, embed_dim, num_heads, hidden_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Ensure the batch size matches the data\n",
    "if x.size(0) < batch_size:\n",
    "    raise ValueError(f\"Batch size {batch_size} exceeds the number of samples {x.size(0)}\")\n",
    "\n",
    "# Select a batch (for simplicity, use the entire data if batch_size matches)\n",
    "x_batch = x[:batch_size]  # Shape: (batch_size, seq_length)\n",
    "\n",
    "# Forward pass\n",
    "logits = model(x_batch)\n",
    "print(\"Logits:\\n\", logits)\n",
    "\n",
    "# Sample target labels (ensure they match the batch size)\n",
    "targets = torch.tensor([0, 1, 0, 0, 1], dtype=torch.long)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(logits, targets)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Backward pass and optimization step\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961090ec",
   "metadata": {},
   "source": [
    "**Explanation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ee56e",
   "metadata": {},
   "source": [
    "- Binning:\n",
    "    - pd.cut: This function discretizes continuous data into specified bins. By setting bins=vocab_size, you ensure that all values are mapped to integers between 0 and vocab_size - 1.\n",
    "    - Flattening and Reshaping: Since pd.cut operates on a 1D array, flatten the DataFrame values, apply binning, and then reshape back to the original DataFrame shape.\n",
    "\n",
    "- Model Adjustments:\n",
    "    - Batch Size Handling: Ensure that the batch_size does not exceed the number of available samples. In this example, since the data covers 6 days, setting batch_size=5 is acceptable.\n",
    "    - Transformer Input Shape: PyTorch's TransformerEncoder expects input in the shape (seq_length, batch_size, embed_dim). Hence, use permute to rearrange dimensions accordingly.\n",
    "\n",
    "- Target Labels:\n",
    "- Ensure that the targets tensor length matches the batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08e3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
